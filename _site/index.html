<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>1st Workshop on Cooperative Intelligence for Embodied AI | ECCV 2024 Workshop</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="1st Workshop on Cooperative Intelligence for Embodied AI" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="ECCV 2024 Workshop" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="1st Workshop on Cooperative Intelligence for Embodied AI" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"1st Workshop on Cooperative Intelligence for Embodied AI","name":"ECCV 2024 Workshop","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="ECCV 2024 Workshop" />
</head>
<body><header class="site-header">
  <div class="wrapper"><!-- <a class="site-title" rel="author" href="/">ECCV 2024 Workshop</a> --><a class="site-title" rel="author" href="/">ECCV 2024 Workshop</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/">Home</a><a class="page-link" href="/speakers/">Speakers</a><a class="page-link" href="/schedule/">Schedule</a><a class="page-link" href="/callforpapers/">Call for papers</a><a class="page-link" href="/organizers/">Organizers</a><a class="page-link" href="/contactus/">Contact Us</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 class="page-heading" align="center"><p style="font-size: 1.3em">1st Workshop on Cooperative Intelligence for Embodied AI</p></h1><p style="text-align: center;"><font size="5"><a href="https://eccv2024.ecva.net/">The 18th European Conference on Computer Vision 2024 in Milan, Italy</a>
</font></p>
<p style="text-align: center;"><font size="5">September 30th, 2024</font></p>
<!-- <p style="text-align: center;"><font size="5">Location: Sequoia 1</font></p> -->
<!-- <p style="text-align: center;"><font size="4">[<a href="">Youtube</a>] [<a href="https://discord.gg/yqjTtBmUJC">Discord</a>] [<a href="https://openreview.net/group?id=IEEE.org/2024/ICRA/Workshop/WIHR">OpenReview</a>]</font></p> -->
<!-- <p style="text-align: center;"><font size="4">Recording available at: <a href="">Youtube</a></font></p> -->

<p>This workshop focuses on cooperative intelligence within multi-agent autonomous systems. The first session will primarily discuss “<strong>Multi-Agent Autonomous Systems Meet Foundation Models: Challenges and Futures</strong>”.</p>

<p>The progress in artificial intelligence has propelled the development of Embodied AI, particularly in autonomous driving and robotics. However, achieving autonomy in complex and open environments remains challenging for individual agents. This is where cooperative intelligence comes in, a paradigm where agents collaborate and interact with infrastructure equipment to handle diverse tasks effectively. In autonomous driving, the availability of datasets and breakthrough algorithms has spurred research interest in cooperative autonomous driving. Through Vehicle-to-Everything (V2X) interactions including Vehicleto-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I), autonomous vehicles can improve their perception and safety capabilities, surpassing the limitations of single-vehicle autonomy and paving the way for widespread adoption. Similarly, in robotics, recent advancements in multi-agent systems offer a cost-effective solution for exploring unknown environments. These breakthroughs empower robots to assist humans in navigating challenging tasks within open environments. Despite notable progress, the challenges of coordinating multi-agent systems remain insufficiently explored. Issues like determining what information to transmit, how to transmit it, and how to fuse information across different task levels (perception, prediction, planning, etc.) pose practical deployment hurdles. Recent breakthroughs in large language models and foundational models offer a promising avenue for addressing these challenges. This workshop, complementing existing workshops focused on individual intelligence in autonomous driving and robotics, introduces a fresh perspective. It fosters discussions on a comprehensive, system-level approach to cooperative framework design for autonomous agents across diverse domains.</p>

<p><img src="/assets/img/banner.jpg" alt="" /></p>

<p>Specifically, this workshop aims to address the following issues regarding cooperative intelligence:</p>
<ul>
  <li><strong>The value and prospects</strong> of cooperative intelligence for autonomous driving and robotics.</li>
  <li><strong>The challenges and recent advancements</strong> in cooperative intelligence for multi-agent autonomous systems.</li>
  <li><strong>The foundation models</strong> for multi-agent autonomous systems.</li>
</ul>

<p> 
 </p>

<h3 id="call-for-papers">Call for papers</h3>

<p><strong>Submission Topics</strong></p>

<p>We invite submissions including but not limited to the following topics:</p>

<ul>
  <li>Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I)</li>
  <li>Multi-agent Robotic System / Swarm Robots</li>
  <li>Communication-Efficient Cooperative Perception</li>
  <li>Cooperative Motion Prediction and Decision-Making</li>
  <li>End-to-End Cooperative Policy Learning</li>
  <li>Efficient and Intelligent V2X Communication</li>
  <li>Data Security and Privacy in Multi-agent Communication</li>
  <li>Multi-Robot Exploration and Mapping</li>
  <li>Intelligent Transportation System</li>
  <li>Large Language Model (LLM)-assisted Cooperative System</li>
  <li>Foundation models for Cooperative System</li>
  <li>Datasets and Benchmarks for Cooperative Learning</li>
</ul>

<p><strong>Important Dates</strong></p>
<ul>
  <li>
    <p><strong>Paper submission open</strong>: June 1st, 2024</p>
  </li>
  <li>
    <p><strong>Paper submission deadline</strong>: <del>July 25th, 2024 (PST)</del> August 10th, 2024 (PST)</p>
  </li>
  <li>
    <p><strong>Notification of acceptance</strong>: <del>August 8th, 2024</del> August 15th, 2024</p>
  </li>
  <li>
    <p><strong>Camera ready</strong>: <del>August 15th, 2024 (PST)</del> August 22nd, 2024 (PST)</p>
  </li>
  <li>
    <p><strong>Workshop date</strong>: PM of Sep 30th, 2024 (See <a href="https://eccv2024.ecva.net/Conferences/2024/Workshops">ECCV Workshop Lists</a>)</p>
  </li>
</ul>

<p><strong>Submission Guidance</strong></p>
<ul>
  <li>
    <p><strong>Submission portal</strong>: <a href="https://openreview.net/group?id=thecvf.com/ECCV/2024/Workshop/MAAS">ECCV 2024 Workshop Co-Intelligence (OpenReview)</a>.</p>
  </li>
  <li>
    <p><strong>Submission format</strong>:
Submissions must follow the ECCV 2024 template (<a href="https://eccv2024.ecva.net/Conferences/2024/SubmissionPolicies">here</a>) and will be peer-reviewed in a double-blind manner. Submission must be no more than 14 pages (excluding references). By default, accepted papers will be included in the ECCV workshop proceedings. Accepted papers will be presented in the form of posters, with several papers being selected for spotlight sessions.</p>
  </li>
</ul>

<p><strong>Important Notes</strong></p>

<p>The accepted papers will be featured in the proceedings, in a separate volume. The <b><font color="red">Best Paper Award</font></b> and <b><font color="red">Best Paper Nomination</font></b> will receive a cash prize of <b>500 US dollars</b> and <b>300 US dollars</b>, respectively.</p>

<p><strong>Contact</strong></p>

<p>If you have any questions, please contact us at: coop-intelligence@googlegroups.com or yuhaibao94@gmail.com.</p>

<p> 
 </p>
<h4 id="accepted-papers">Accepted papers</h4>

<p>We accept 20 papers from submissions, including 6 orals. Paper list is as following: <strong>Title-Authors-Oral/Poster</strong></p>

<ul>
  <li>Multi-agent Collaborative Perception for Robotic Fleet: A Systematic Review - Apoorv Singh, Gaurav Raut, Alka Choudhary - Poster</li>
  <li>RP3D: A Roadside Perception Framework for 3D Object Detection via Multi-View Sensor Fusion - Shaowu Zheng, Ruyi Huang, Yuan Ji, Ming Ye, Weihua Li - Oral</li>
  <li>StreamLTS: Query-based Temporal-Spatial LiDAR Fusion for Cooperative Object Detection - Yunshuang Yuan, Monika Sester - Poster</li>
  <li>GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest - Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo - Poster</li>
  <li>SC-Track: State Transition and Constrained Non-negative Matrix Factorization for Multi-Camera Multi-Target Tracking - Xiaolong Yang, Xuting Duan, Jianshan Zhou, Chunmian Lin, XuHan - Poster</li>
  <li>Gen-Swarms: Adapting Deep Generative Models to Swarms of Drones - Carlos Plou, Pablo Pueyo, Ruben Martinez-Cantin, Mac Schwager, Ana C. Murillo, Eduardo Montijano - Oral</li>
  <li>VICooper: A Practical Vehicle-Infrastructure Cooperative Perception Framework for Autonomous Driving - Shaowu Zheng, Ming Ye, Yuan Ji, Ruyi Huang, Weihua Li - Poster</li>
  <li>MEDCO: Medical Education Copilots Based on A Multi-Agent Framework - Hao Wei, Jianing Qiu, Haibao Yu, Wu Yuan - Poster</li>
  <li>V2X-Based Decentralized Singular Value Decomposition in Dynamic Vehicular Environment - Jianxin Zhao, Min-Bin Lin, Alexey Vinel - Poster</li>
  <li>LLaMAPed: Multi-modal Pedestrian Crossing Intention Prediction - Je-Seok Ham, Sunghun Kim, Jia Huang, Peng Jiang, Jinyoung Moon, Srikanth Saripalli, Changick Kim - Poster</li>
  <li>Optimization of Layer Skipping and Frequency Scaling for Convolutional Neural Networks under Latency Constraint - Minh David Thao Chan, Ruoyu Zhao, Yukuan Jia, Ruiqing Mao, Sheng Zhou - Poster</li>
  <li>An Infrastructure-based Localization Method for Articulated Vehicles - Alberto Justo, Iker Pacho, Javier Araluce, Jesus Murgoitio, Luis M. Bergasa - Poster</li>
  <li>HEAD: A Bandwidth-Efficient Cooperative Perception Approach for Heterogeneous Connected and Autonomous Vehicles - Deyuan Qu, Qi Chen, Yongqi Zhu, Yihao Zhu, Sergei S. Avedisov, Song Fu, Qing Yang - Poster</li>
  <li>Rethinking the Role of Infrastructure in Collaborative Perception - Hyunchul Bae, Minhee Kang, Minwoo Song, Heejin Ahn - Oral</li>
  <li>Empowering Autonomous Shuttles with Next-Generation Infrastructure - Sven Ochs, Melih Yazgan, Rupert Polley, Albert Schotschneider, Stefan Orf, Marc Uecker, Maximilian Zipfl, Julian Burger, Abhishek Vivekanandan, Jennifer Amritzer, Marc Rene Zofka, J. Marius Zöllner - Oral</li>
  <li>MAPPO-PIS: A Multi-Agent Proximal Policy Optimization Method with Prior Intent Sharing for CAVs’ Cooperative Decision-Making - Yicheng Guo, Jiaqi Liu, Rongjie Yu, Peng Hang, Jian Sun - Oral</li>
  <li>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version) - Yao Mu, Tianxing Chen, Shijia Peng, Zanxin Chen, Zeyu Gao, Yude Zou, Lunkai Lin, Zhiqiang Xie - Oral</li>
  <li>iIPPC-V2X: Multi-modality Fusion Perception System for Cooperative Vehicle Infrastructure System with Self-supervised Learning - Guoyu Zhang, Rongjie Yu, Jian Sun, Peng Hang - Poster</li>
  <li>Non-verbal Interaction and Interface with a Quadruped Robot using Body and Hand Gestures: Design and User Experience Evaluation - Soohyun Shin, Trevor Evetts, Hunter Saylor, Hyunji Kim, Soojin Woo, Wonhwha Rhee, Seong-Woo Kim - Poster</li>
  <li>Transfer Learning from Simulated to Real Scenes for Monocular 3D Object Detection - Sondos Mohamed, Walter Zimmer, Ross Greer, Ahmed Alaaeldin Ghita, Modesto Castrillón-Santana, Mohan Trivedi, Alois Knoll, Salvatore Mario Carta, Mirko Marras - Poster</li>
</ul>

<p> 
 </p>
<h3 id="invited-speakers-tbd">Invited Speakers (TBD)</h3>
<p> </p>

<div class="grid">
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/jiaqi_ma.jpg" width="200" />
        <figcaption><b><a href="https://mobility-lab.seas.ucla.edu/about/">Jiaqi Ma</a></b><br />UCLA, USA</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/alois_knoll.png" width="200" /> 
        <figcaption><b><a href="https://www.professoren.tum.de/en/knoll-alois-christian/">Alois C. Knoll</a></b><br />TUM, Germany</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/hang_qiu.jpg" width="200" />
        <figcaption><b><a href="https://hangqiu.github.io/">Hang Qiu</a></b><br />University of California, Riverside, USA</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\speakers\Guillaume_Sartoretti.jpg" width="200" />
        <figcaption><b><a href="https://cde.nus.edu.sg/me/staff/sartoretti-guillaume-a/">Guillaume Sartoretti</a></b><br />NUS, Singapore</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\speakers\zhou_sheng.jpg" width="200" />
        <figcaption><b><a href="http://network.ee.tsinghua.edu.cn/shengzhou/">Sheng Zhou
</a></b><br />Tsinghua University, China</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\speakers\Amanda_Prorok.jpg" width="200" />
        <figcaption><b><a href="https://www.cst.cam.ac.uk/people/asp45">Amanda Prorok</a></b><br />Cambridge University, UK</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\speakers\zsolt_kira.jpg" width="200" />
        <figcaption><b><a href="https://faculty.cc.gatech.edu/~zk15/">Zsolt Kira</a></b><br />Gatech, USA</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\speakers\Manabu_Tsukada.jpg" width="200" />
        <figcaption><b><a href="https://tlab.hongo.wide.ad.jp/People/manabu-tsukada/">Manabu Tsukada</a></b><br />The University of Tokyo, Japan</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\speakers\marco_pavone.jpeg" width="200" />
        <figcaption><b><a href="https://web.stanford.edu/~pavone/">Marco Pavone</a></b><br />Standford University, USA</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\speakers\Jingwei_Ji.jpg" width="200" />
        <figcaption><b><a href="https://jingweij.github.io/">Jingwei Ji</a></b><br />Waymo, USA</figcaption>
        </figure>
    </div>
</div>

<p> </p>

<h3 id="organizers">Organizers</h3>
<p> </p>

<div class="grid">
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\haibao_yu1.jpg" width="200" />
        <figcaption><b><a href="https://www.linkedin.com/in/haibao-yu-152221118">Haibao Yu</a></b><br />University of Hong Kong &amp; Tsinghua University</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\jianing_qiu.jpeg" width="200" /> 
        <figcaption><b><a href="https://www.linkedin.com/in/jianing-qiu-91a802224/">Jianing Qiu</a></b><br />CUHK</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\jiankai_sun.png" width="200" />
        <figcaption><b><a href="http://web.stanford.edu/~jksun/">Jiankai Sun</a></b><br />Stanford University</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\li_chen.png" width="200" />
        <figcaption><b><a href="https://scholar.google.com/citations?hl=en&amp;user=ulZxvY0AAAAJ">Li Chen</a></b><br />Shanghai AI Lab</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\walter_zimmer.jpeg" width="200" />
        <figcaption><b><a href="https://walzimmer.github.io/website/">Walter Zimmer</a></b><br />Technical University of Munich</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\mac_schwager.jpg" width="200" />
        <figcaption><b><a href="https://web.stanford.edu/~schwager/">Mac Schwager</a></b><br />Stanford University</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\ping_luo.jpg" width="200" />
        <figcaption><b><a href="http://luoping.me/">Ping Luo</a></b><br />University of Hong Kong</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\ruigang_yang.jpg" width="200" />
        <figcaption><b><a href="https://scholar.google.com/citations?user=yveq40QAAAAJ&amp;hl=en">Ruigang Yang</a></b><br />Inceptio Technology</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\si_liu.jpg" width="200" />
        <figcaption><b><a href="https://colalab.net/">Si Liu</a></b><br />Beihang University</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets\img\organizers\zaiqing_nie.jpg" width="200" />
        <figcaption><b><a href="https://air.tsinghua.edu.cn/en/info/1046/1192.htm">Zaiqing Nie</a></b><br />Tsinghua University</figcaption>
        </figure>
    </div>
</div>

<h3 id="program-committee">Program Committee</h3>
<p> </p>
<div class="grid">
    <div class="gridcom-item">
        <div><p><span>&#8226;&nbsp;</span>Jiaru Zhong (BIT)</p></div>
        <div><p><span>&#8226;&nbsp;</span>Zhenwei Yang (USTB)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://sites.google.com/site/yangliuveronica/">Yang Liu</a> (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://xieenze.github.io/">Enze Xie</a> (HKU)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://scholar.google.com/citations?user=KlHuj2QAAAAJ&amp;hl">Yifeng Shi</a> (Baidu)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://scholar.google.com/citations?user=1H5PwZkAAAAJ&amp;hl">Weitao Zhou</a> (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://phyllish.github.io/">Yue Hu</a> (SJTU)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://shi.buaa.edu.cn/duanxuting/zh_CN/index.htm">Xuting Duan</a> (Beihang University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://liang-zx.github.io/">Zhixuan Liang</a> (HKU)</p></div>
        <div><p><span>&#8226;&nbsp;</span>Chuheng Wei (University of California, Riverside)</p></div>
        <div><p><span>&#8226;&nbsp;</span>Churan Zhi (University of California, San Diego)</p></div>
    </div>
    <div class="gridcom-item">
        <div><p><span>&#8226;&nbsp;</span><a href="https://ry-hao.top/">Ruiyang Hao</a> (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span>Siqi Fan (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://yanwang202199.github.io/">Yan Wang</a> (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://scholar.google.com/citations?user=3qK1cZMAAAAJ&amp;hl">Tuopu Wen</a> (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://yaomarkmu.github.io/">Yao Mu</a> (HKU)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://dingmyu.github.io/">Mingyu Ding</a> (UC Berkeley)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://ieeexplore.ieee.org/author/37086579788">Lipeng Chen</a> (Tencent Robotics X)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://scholar.google.com.hk/citations?hl=en&amp;user=nUyTDosAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Yilun Wang</a> (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span>Deyuan Qu (UNT)</p></div>
        <div><p><span>&#8226;&nbsp;</span>Alberto Justo (Tecnalia Research &amp; Innovation)</p></div>
        <div><p><span>&#8226;&nbsp;</span>Lin Li (KCL)</p></div>
    </div>
    <div class="gridcom-item">
        <div><p><span>&#8226;&nbsp;</span>Wenxian Yang (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://yuexinma.me/">Yuexin Ma</a> (ShanghaiTech University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://sites.google.com/view/guangliangcheng">Guangliang Cheng</a> (UOL)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://rruisong.github.io">Rui Song</a> (TUM)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://air.tsinghua.edu.cn/en/info/1012/1219.htm">Jirui Yuan</a> (Tsinghua University)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://shijianping.me/">Jianping Shi</a> (SenseTime)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://jiachenli94.github.io/">Jiachen Li</a> (University of California, Riverside)</p></div>
        <div><p><span>&#8226;&nbsp;</span>Xiaopan Zhang (University of California, Riverside)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://www.linkedin.com/in/ghayoor-shah/">Ghayoor Shah</a> (University of Central Florida)</p></div>
        <div><p><span>&#8226;&nbsp;</span><a href="https://www.linkedin.com/in/hyunchul-bae-a25a17314/">Hyunchul Bae</a> (KAIST)</p></div>
    </div>
</div>

<h3 id="sponsors">Sponsors</h3>
<p> </p>

<div class="grid">
    <div class="gridorg-item">
        <figure>
        <a href="https://www.didiglobal.com/science/intelligent-driving/"><img src="assets/img/sponsors/didi_logo.jpg" width="500" /></a>
        <!-- <figcaption><b><a href="https://www.didiglobal.com/science/intelligent-driving">DIDI</a></b><br></figcaption> -->
        </figure>
    </div>
</div>



  </div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <!-- <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p> -->
      </div>
      <div class="footer-col">
        <p></p>
        <p></p>
        
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

    <div class="footer-col" align="center">
      <a href="https://github.com/krrish94/sniffle-workshop">Template</a>
    </div>

  </div>

</footer>
</body>

</html>
